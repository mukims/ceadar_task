{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ebbde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pypdf import PdfReader , PdfWriter\n",
    "#from pypdf import \n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from docx import Document as DocxDocument\n",
    "from openpyxl import load_workbook\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "from typing import List\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eabdc44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class config:\n",
    "    files_dir: Path\n",
    "    output_dir: Path\n",
    "    max_tokens : int\n",
    "    token_overlap : int\n",
    "    tokenizer_name : str\n",
    "    embedding_model :str\n",
    "    llm_model :str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d01902d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"./processed/\").mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef97d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir = Path(\".\")\n",
    "\n",
    "CORPUS_PATH = Path(\"./processed/corpus.jsonl\")\n",
    "CHUNKS_PATH = Path(\"./processed/chunks.jsonl\")\n",
    "EMBEDDINGS_PATH = Path(\"./processed/embeddings.npy\")\n",
    "META_PATH = Path(\"./processed/metadata.jsonl\")\n",
    "FAISS_INDEX_PATH = Path(\"./processed/faiss.index\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "605e5a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_documents(files_dir:Path)-> list[Document]:\n",
    "    files_dir = Path(files_dir)\n",
    "\n",
    "    if not files_dir.exists():\n",
    "        raise FileNotFoundError(\"Folder not found..\")\n",
    "    \n",
    "    docs : List[Document] = []\n",
    "\n",
    "    files_required = sorted(\n",
    "        p for p in files_dir.rglob(\"*\") if p.is_file() and p.suffix.lower() in {\".pdf\",\".docx\",\".xlsx\"}\n",
    "    )\n",
    "\n",
    "\n",
    "    for path in files_required:\n",
    "        ext = path.suffix.lower()\n",
    "\n",
    "        try:\n",
    "            if ext ==\".pdf\":\n",
    "                docs.extend(_load_pdf(path))\n",
    "            elif ext == \".docx\":\n",
    "                docs.extend(_load_docx(path))\n",
    "            elif ext == \".xlsx\":\n",
    "                docs.extend(_load_xlsx(path))\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Not loading %s, %s\", path,e)\n",
    "\n",
    "            continue\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e3a2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_pdf(path: Path) -> List[Document]:\n",
    "    \n",
    "    out: List[Document] = []\n",
    "    reader = PdfReader(str(path))\n",
    "\n",
    "    for i, page in enumerate(reader.pages, start=1):\n",
    "        text = (page.extract_text() or \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        out.append(\n",
    "            Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"source\": str(path),\n",
    "                    \"filename\": path.name,\n",
    "                    \"filetype\": \"pdf\",\n",
    "                    \"page\": i,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "    return out\n",
    "\n",
    "\n",
    "def _load_docx(path: Path) -> List[Document]:\n",
    "    out: List[Document] = []\n",
    "    doc = DocxDocument(str(path))\n",
    "\n",
    "    # paragraphs\n",
    "    for pi, para in enumerate(doc.paragraphs, start=1):\n",
    "        text = (para.text or \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        out.append(\n",
    "            Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"source\": str(path),\n",
    "                    \"filename\": path.name,\n",
    "                    \"filetype\": \"docx\",\n",
    "                    \"block\": f\"paragraph:{pi}\",\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # tables\n",
    "    for ti, table in enumerate(doc.tables, start=1):\n",
    "        rows_text: List[str] = []\n",
    "        for row in table.rows:\n",
    "            cells = [ (cell.text or \"\").strip() for cell in row.cells ]\n",
    "            # drop fully empty rows\n",
    "            if any(cells):\n",
    "                rows_text.append(\"\\t\".join(cells))\n",
    "\n",
    "        table_text = \"\\n\".join(rows_text).strip()\n",
    "        if table_text:\n",
    "            out.append(\n",
    "                Document(\n",
    "                    page_content=table_text,\n",
    "                    metadata={\n",
    "                        \"source\": str(path),\n",
    "                        \"filename\": path.name,\n",
    "                        \"filetype\": \"docx\",\n",
    "                        \"block\": f\"table:{ti}\",\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _load_xlsx(path: Path) -> List[Document]:\n",
    "    out: List[Document] = []\n",
    "\n",
    "    wb = load_workbook(filename=str(path), read_only=True, data_only=True)\n",
    "\n",
    "    for sheet_name in wb.sheetnames:\n",
    "        ws = wb[sheet_name]\n",
    "\n",
    "        lines: List[str] = []\n",
    "        for row in ws.iter_rows(values_only=True):\n",
    "            # Convert row to strings, drop empty cells\n",
    "            row_vals = [(\"\" if c is None else str(c)).strip() for c in row]\n",
    "\n",
    "            # skip fully empty rows\n",
    "            if not any(row_vals):\n",
    "                continue\n",
    "\n",
    "            lines.append(\"\\t\".join(row_vals))\n",
    "\n",
    "        sheet_text = \"\\n\".join(lines).strip()\n",
    "        if sheet_text:\n",
    "            out.append(\n",
    "                Document(\n",
    "                    page_content=sheet_text,\n",
    "                    metadata={\n",
    "                        \"source\": str(path),\n",
    "                        \"filename\": path.name,\n",
    "                        \"filetype\": \"xlsx\",\n",
    "                        \"sheet\": sheet_name,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df088ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 137 docs -> 240 chunks\n",
      "{'source': 'Attention_is_all_you_need (1) (3) (1).pdf', 'filename': 'Attention_is_all_you_need (1) (3) (1).pdf', 'filetype': 'pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    ")\n",
    "\n",
    "raw_docs = load_documents(files_dir)\n",
    "chunks = text_splitter.split_documents(raw_docs)\n",
    "\n",
    "print(f\"Loaded {len(raw_docs)} docs -> {len(chunks)} chunks\")\n",
    "print(chunks[0].metadata)\n",
    "#print(chunks[0].page_content[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46e56403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_586184/1822630081.py:11: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"chroma_db\",\n",
    ")\n",
    "vectordb.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95fdadc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentence-transformers\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"chroma_db\",\n",
    ")\n",
    "vectordb.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a425b123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'block': 'paragraph:87', 'source': 'EU AI Act Doc (1) (3) (1).docx', 'filename': 'EU AI Act Doc (1) (3) (1).docx', 'filetype': 'docx'}\n",
      "Downstream providers can lodge a complaint regarding the upstream providers infringement to the AI Office.\n",
      "-----\n",
      "{'block': 'paragraph:87', 'filename': 'EU AI Act Doc (1) (3) (1).docx', 'source': 'EU AI Act Doc (1) (3) (1).docx', 'filetype': 'docx'}\n",
      "Downstream providers can lodge a complaint regarding the upstream providers infringement to the AI Office.\n",
      "-----\n",
      "{'block': 'paragraph:70', 'filetype': 'docx', 'source': 'EU AI Act Doc (1) (3) (1).docx', 'filename': 'EU AI Act Doc (1) (3) (1).docx'}\n",
      "Establish a policy to respect the Copyright Directive.\n",
      "-----\n",
      "{'block': 'paragraph:70', 'source': 'EU AI Act Doc (1) (3) (1).docx', 'filename': 'EU AI Act Doc (1) (3) (1).docx', 'filetype': 'docx'}\n",
      "Establish a policy to respect the Copyright Directive.\n",
      "-----\n",
      "{'filetype': 'docx', 'source': 'EU AI Act Doc (1) (3) (1).docx', 'filename': 'EU AI Act Doc (1) (3) (1).docx', 'block': 'paragraph:94'}\n",
      "12 months for GPAI.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "results = retriever.invoke(\"What is the refund policy?\")\n",
    "for r in results:\n",
    "    print(r.metadata)\n",
    "    print(r.page_content[:200])\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18856d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[32m      4\u001b[39m llm = ChatOpenAI(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4.1-mini\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# example\u001b[39;00m\n\u001b[32m      5\u001b[39m qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.chains'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5fa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain.chains (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain.chains\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f28432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
