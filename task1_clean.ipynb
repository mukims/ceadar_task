{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac56babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import faiss\n",
    "from docx import Document\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366e55d",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Define the immutable config dataclass for all pipeline parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8510ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class config:\n",
    "    files_dir: Path\n",
    "    output_dir: Path\n",
    "    max_tokens: int\n",
    "    token_overlap: int\n",
    "    tokenizer_name: str\n",
    "    embedding_model: str\n",
    "    llm_model: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f98072",
   "metadata": {},
   "source": [
    "## Path Helpers\n",
    "Output file path generators with automatic directory creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b69022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_path(output_dir: Path) -> Path:\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return output_dir / \"corpus.jsonl\"\n",
    "\n",
    "\n",
    "def chunks_path(output_dir: Path) -> Path:\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return output_dir / \"chunks.jsonl\"\n",
    "\n",
    "\n",
    "def embeddings_path(output_dir: Path) -> Path:\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return output_dir / \"embeddings.npy\"\n",
    "\n",
    "\n",
    "def metadata_path(output_dir: Path) -> Path:\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return output_dir / \"metadata.jsonl\"\n",
    "\n",
    "\n",
    "def faiss_index_path(output_dir: Path) -> Path:\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return output_dir / \"faiss_index.index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8fda9",
   "metadata": {},
   "source": [
    "## Stage 1: PDF Extraction\n",
    "Extract text from all PDF pages with graceful error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdfs(files_dir: Path) -> str:\n",
    "    \"\"\"Extract text from all pages in a PDF file.\"\"\"\n",
    "    reader = PdfReader(str(files_dir))\n",
    "    pages = [page.extract_text() or \"\" for page in reader.page]\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "\n",
    "def extract_text_from_docx(files_dir: Path) -> str:\n",
    "    \"\"\"Extract text from a DOCX file.\"\"\"\n",
    "    doc = Document(str(files_dir))\n",
    "    paragraphs = [para.text for para in doc.paragraphs]\n",
    "    # Also extract from tables if present\n",
    "    for table in doc.tables:\n",
    "        for row in table.rows:\n",
    "            for cell in row.cells:\n",
    "                paragraphs.append(cell.text)\n",
    "    return \"\\n\".join(paragraphs)\n",
    "\n",
    "\n",
    "def extract_text_from_excel(files_dir: Path) -> str:\n",
    "    \"\"\"Extract text from an Excel file.\"\"\"\n",
    "    workbook = load_workbook(str(files_dir))\n",
    "    text_content = []\n",
    "    \n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "        text_content.append(f\"\\n=== Sheet: {sheet_name} ===\\n\")\n",
    "        \n",
    "        for row in sheet.iter_rows(values_only=True):\n",
    "            row_text = [str(cell) if cell is not None else \"\" for cell in row]\n",
    "            text_content.append(\"\\t\".join(row_text))\n",
    "    \n",
    "    return \"\\n\".join(text_content)\n",
    "\n",
    "\n",
    "def extract_text_by_filetype(file_path: Path) -> str:\n",
    "    \"\"\"Automatically detect file type and extract text.\"\"\"\n",
    "    suffix = file_path.suffix.lower()\n",
    "    \n",
    "    if suffix == \".pdf\":\n",
    "        return extract_text_from_pdfs(file_path)\n",
    "    elif suffix == \".docx\":\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif suffix in [\".xlsx\", \".xls\"]:\n",
    "        return extract_text_from_excel(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {suffix}. Supported: .pdf, .docx, .xlsx, .xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b7b3a",
   "metadata": {},
   "source": [
    "## Stage 2: Corpus Creation\n",
    "Save extracted text to corpus.jsonl for persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbcc6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(cfg: config, pdf_file: Path = None, input_files: list = None) -> str:\n",
    "    \"\"\"Create corpus from one or multiple files (PDF, DOCX, XLSX).\"\"\"\n",
    "    corpus_text = \"\"\n",
    "    corpus_metadata = []\n",
    "    \n",
    "    # Handle single file or multiple files\n",
    "    files_to_process = []\n",
    "    if input_files:\n",
    "        files_to_process = input_files if isinstance(input_files, list) else [input_files]\n",
    "    elif pdf_file:\n",
    "        files_to_process = [pdf_file]\n",
    "    else:\n",
    "        raise ValueError(\"Either pdf_file or input_files must be provided\")\n",
    "    \n",
    "    # Extract text from all files\n",
    "    for file_path in files_to_process:\n",
    "        file_path = Path(file_path)\n",
    "        print(f\"Processing {file_path.name}...\")\n",
    "        \n",
    "        text = extract_text_by_filetype(file_path)\n",
    "        corpus_text += f\"\\n\\n[SOURCE: {file_path.name}]\\n{text}\"\n",
    "        \n",
    "        corpus_metadata.append({\n",
    "            \"source\": file_path.name,\n",
    "            \"file_type\": file_path.suffix.lower(),\n",
    "            \"length\": len(text)\n",
    "        })\n",
    "    \n",
    "    # Save corpus with metadata\n",
    "    corpus_file = corpus_path(cfg.output_dir)\n",
    "    with open(corpus_file, 'w') as f:\n",
    "        json.dump({\n",
    "            \"files\": corpus_metadata,\n",
    "            \"text\": corpus_text,\n",
    "            \"total_length\": len(corpus_text)\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"Corpus saved to {corpus_file} ({len(corpus_text)} characters from {len(files_to_process)} file(s))\")\n",
    "    return corpus_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbf32a",
   "metadata": {},
   "source": [
    "## Stage 3: Tokenization & Chunking\n",
    "Split corpus into overlapping chunks with token-level precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bdb8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(cfg: config, corpus_text: str, source_file: str, page: int = 0) -> list:\n",
    "    \"\"\"Create overlapping chunks respecting max_tokens and token_overlap.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer_name)\n",
    "    \n",
    "    # Tokenize full corpus\n",
    "    tokens = tokenizer.encode(corpus_text)\n",
    "    print(f\"Total tokens: {len(tokens)}\")\n",
    "    \n",
    "    chunks = []\n",
    "    chunk_id = 0\n",
    "    start_token = 0\n",
    "    \n",
    "    while start_token < len(tokens):\n",
    "        # Define chunk boundaries\n",
    "        end_token = min(start_token + cfg.max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[start_token:end_token]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        \n",
    "        chunks.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"text\": chunk_text,\n",
    "            \"source_file\": source_file,\n",
    "            \"page\": page,\n",
    "            \"token_count\": len(chunk_tokens),\n",
    "            \"start_token\": start_token,\n",
    "            \"end_token\": end_token\n",
    "        })\n",
    "        \n",
    "        # Move to next chunk with overlap\n",
    "        start_token = end_token - cfg.token_overlap if cfg.token_overlap > 0 else end_token\n",
    "        chunk_id += 1\n",
    "    \n",
    "    # Save chunks\n",
    "    chunks_file = chunks_path(cfg.output_dir)\n",
    "    with open(chunks_file, 'w') as f:\n",
    "        for chunk in chunks:\n",
    "            f.write(json.dumps(chunk) + '\\n')\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks, saved to {chunks_file}\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57737081",
   "metadata": {},
   "source": [
    "## Stage 4: Embedding Generation\n",
    "Convert chunks to semantic vectors using SentenceTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34bb6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(cfg: config, chunks: list) -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for all chunks.\"\"\"\n",
    "    model = SentenceTransformer(cfg.embedding_model)\n",
    "    \n",
    "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    embeddings = model.encode(chunk_texts, convert_to_numpy=True)\n",
    "    \n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Save embeddings\n",
    "    embeddings_file = embeddings_path(cfg.output_dir)\n",
    "    np.save(embeddings_file, embeddings)\n",
    "    print(f\"Embeddings saved to {embeddings_file}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_file = metadata_path(cfg.output_dir)\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            f.write(json.dumps({\n",
    "                \"chunk_id\": chunk[\"chunk_id\"],\n",
    "                \"embedding_index\": i,\n",
    "                \"embedding_dim\": embeddings.shape[1],\n",
    "                \"model\": cfg.embedding_model\n",
    "            }) + '\\n')\n",
    "    print(f\"Metadata saved to {metadata_file}\")\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa76756",
   "metadata": {},
   "source": [
    "## Stage 5: FAISS Indexing\n",
    "Build vector similarity index for fast retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cded8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(cfg: config, embeddings: np.ndarray) -> faiss.Index:\n",
    "    \"\"\"Create FAISS index for vector similarity search.\"\"\"\n",
    "    embedding_dim = embeddings.shape[1]\n",
    "    \n",
    "    # Create index (L2 distance)\n",
    "    index = faiss.IndexFlatL2(embedding_dim)\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "    \n",
    "    print(f\"FAISS index created with {index.ntotal} vectors\")\n",
    "    \n",
    "    # Save index\n",
    "    index_file = faiss_index_path(cfg.output_dir)\n",
    "    faiss.write_index(index, str(index_file))\n",
    "    print(f\"FAISS index saved to {index_file}\")\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17109e4a",
   "metadata": {},
   "source": [
    "## Stage 6: Similarity Search & Retrieval\n",
    "Query the index to retrieve relevant chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79707bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(cfg: config, query: str, k: int = 5) -> list:\n",
    "    \"\"\"Search for top-k similar chunks.\"\"\"\n",
    "    # Load model and index\n",
    "    model = SentenceTransformer(cfg.embedding_model)\n",
    "    index = faiss.read_index(str(faiss_index_path(cfg.output_dir)))\n",
    "    \n",
    "    # Encode query\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)[0]\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype(np.float32)\n",
    "    \n",
    "    # Search\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Load chunks and return top-k\n",
    "    results = []\n",
    "    chunks_file = chunks_path(cfg.output_dir)\n",
    "    chunks_list = []\n",
    "    with open(chunks_file, 'r') as f:\n",
    "        chunks_list = [json.loads(line) for line in f]\n",
    "    \n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        results.append({\n",
    "            \"rank\": i + 1,\n",
    "            \"distance\": float(distances[0][i]),\n",
    "            \"chunk\": chunks_list[idx]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104682bf",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "Run the complete RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Single file (PDF, DOCX, or XLSX)\n",
    "# cfg = config(\n",
    "#     files_dir=Path(\"./Attention_is_all_you_need.pdf\"),\n",
    "#     output_dir=Path(\"./output\"),\n",
    "#     max_tokens=512,\n",
    "#     token_overlap=50,\n",
    "#     tokenizer_name=\"gpt2\",\n",
    "#     embedding_model=\"all-MiniLM-L6-v2\",\n",
    "#     llm_model=\"gpt2\"\n",
    "# )\n",
    "\n",
    "# corpus_text = create_corpus(cfg, pdf_file=cfg.files_dir)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "# Example 2: Multiple files of different types\n",
    "# cfg = config(\n",
    "#     files_dir=Path(\"./documents\"),\n",
    "#     output_dir=Path(\"./output_multi\"),\n",
    "#     max_tokens=512,\n",
    "#     token_overlap=50,\n",
    "#     tokenizer_name=\"gpt2\",\n",
    "#     embedding_model=\"all-MiniLM-L6-v2\",\n",
    "#     llm_model=\"gpt2\"\n",
    "# )\n",
    "\n",
    "# input_files = [\n",
    "#     Path(\"./Attention_is_all_you_need.pdf\"),\n",
    "#     Path(\"./EU AI Act Doc.docx\"),\n",
    "#     Path(\"./Inflation Calculator.xlsx\")\n",
    "# ]\n",
    "\n",
    "# # Pipeline execution with multiple files\n",
    "# corpus_text = create_corpus(cfg, input_files=input_files)\n",
    "# chunks = create_chunks(cfg, corpus_text, source_file=\"multi-document\", page=0)\n",
    "# embeddings = create_embeddings(cfg, chunks)\n",
    "# index = create_faiss_index(cfg, embeddings)\n",
    "\n",
    "# # Search example\n",
    "# results = search_similar_chunks(cfg, \"What is inflation?\", k=3)\n",
    "# for result in results:\n",
    "#     print(f\"\\nRank {result['rank']} (distance: {result['distance']:.4f})\")\n",
    "#     print(result['chunk']['text'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c4015",
   "metadata": {},
   "source": [
    "## Practical Example - Ready to Run\n",
    "Complete working example using the actual files in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa99d847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration ready\n",
      "  Output directory: rag_output\n",
      "  Max tokens per chunk: 512\n",
      "  Token overlap: 50\n",
      "  Embedding model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Step 1: Configure the pipeline\n",
    "cfg = config(\n",
    "    files_dir=Path(\"./Attention_is_all_you_need (1) (3) (1).pdf\"),\n",
    "    output_dir=Path(\"./rag_output\"),\n",
    "    max_tokens=512,\n",
    "    token_overlap=50,\n",
    "    tokenizer_name=\"gpt2\",\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    llm_model=\"gpt2\"\n",
    ")\n",
    "\n",
    "print(\"✓ Configuration ready\")\n",
    "print(f\"  Output directory: {cfg.output_dir}\")\n",
    "print(f\"  Max tokens per chunk: {cfg.max_tokens}\")\n",
    "print(f\"  Token overlap: {cfg.token_overlap}\")\n",
    "print(f\"  Embedding model: {cfg.embedding_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbffeb61",
   "metadata": {},
   "source": [
    "### Stage 1: Extract & Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f694be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Attention_is_all_you_need (1) (3) (1).pdf...\n",
      "Corpus saved to rag_output/corpus.jsonl (39683 characters from 1 file(s))\n"
     ]
    }
   ],
   "source": [
    "corpus_text = create_corpus(cfg, pdf_file=cfg.files_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c22d637",
   "metadata": {},
   "source": [
    "### Stage 2: Create Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f47493b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263720f838fa430495a12b4b4f14f36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80572783eb7f49b0a3903342bcbe7a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f4fc247cd44e39b17489de83de32c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39793c4ffae4dabb1fc04948a479391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f77877d274142a2b3799b2c87e8bca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10599 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 10599\n"
     ]
    }
   ],
   "source": [
    "chunks = create_chunks(cfg, corpus_text, source_file=cfg.files_dir.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbb56d",
   "metadata": {},
   "source": [
    "### Stage 3: Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d5f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = create_embeddings(cfg, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba6f06",
   "metadata": {},
   "source": [
    "### Stage 4: Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = create_faiss_index(cfg, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd774e39",
   "metadata": {},
   "source": [
    "### Stage 5: Query the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the attention mechanism?\"\n",
    "results = search_similar_chunks(cfg, query, k=3)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"QUERY: {query}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    rank = result['rank']\n",
    "    distance = result['distance']\n",
    "    chunk_text = result['chunk']['text'][:300]\n",
    "    source = result['chunk']['source_file']\n",
    "    \n",
    "    print(f\"[Rank {rank}] Distance: {distance:.4f} | Source: {source}\")\n",
    "    print(f\"{chunk_text}...\")\n",
    "    print(\"-\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a93f7",
   "metadata": {},
   "source": [
    "## Multi-Document RAG Example\n",
    "Process both Attention & Deepseek PDFs together for unified semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec82051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure for multi-document RAG\n",
    "cfg_multi = config(\n",
    "    files_dir=Path(\"./\"),\n",
    "    output_dir=Path(\"./rag_output_multi\"),\n",
    "    max_tokens=512,\n",
    "    token_overlap=50,\n",
    "    tokenizer_name=\"gpt2\",\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    llm_model=\"gpt2\"\n",
    ")\n",
    "\n",
    "# Process both PDFs\n",
    "input_files = [\n",
    "    Path(\"./Attention_is_all_you_need (1) (3) (1).pdf\"),\n",
    "    Path(\"./Deepseek-r1 (1).pdf\")\n",
    "]\n",
    "\n",
    "print(\"✓ Multi-document configuration ready\")\n",
    "print(f\"  Processing {len(input_files)} PDFs:\")\n",
    "for f in input_files:\n",
    "    print(f\"    - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and create unified corpus\n",
    "corpus_text_multi = create_corpus(cfg_multi, input_files=input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95391a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunks, embeddings, and index\n",
    "chunks_multi = create_chunks(cfg_multi, corpus_text_multi, source_file=\"multi-pdf\", page=0)\n",
    "embeddings_multi = create_embeddings(cfg_multi, chunks_multi)\n",
    "index_multi = create_faiss_index(cfg_multi, embeddings_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a56e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search across both documents\n",
    "query = \"What is reasoning and model architecture?\"\n",
    "results_multi = search_similar_chunks(cfg_multi, query, k=5)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MULTI-DOCUMENT QUERY: {query}\")\n",
    "print(f\"Searching across {len(input_files)} PDFs\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "for result in results_multi:\n",
    "    rank = result['rank']\n",
    "    distance = result['distance']\n",
    "    chunk_text = result['chunk']['text'][:250]\n",
    "    source = result['chunk']['source_file']\n",
    "    \n",
    "    print(f\"[Rank {rank}] Distance: {distance:.4f} | From: {source}\")\n",
    "    print(f\"{chunk_text}...\")\n",
    "    print(\"-\" * 70 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
